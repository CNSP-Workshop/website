<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>Resources | CNSP2022</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="keywords" content="Your keywords">
<meta name="author" content="Your name">
<link rel="stylesheet" href="styles.css" />
</head>
<body id="page2">
<div class="bg-main">
  <!--==============================header=================================-->
 <div class="header_bg">
  <div class="header">

      <div class="main">
        <h1><a href="#"><img src="images/CNSP_Logo.gif" alt="" style="height:150px;"></a></h1>

	  </div>

        <nav>
          <ul class="sf-menu">
            <li><a href="index.html">Home </a></li>
            <li><a href="registration.html">Registration </a></li>
            <li><a href="schedule.html">Schedule </a></li>
            <li class="current"><a href="resources.html">Resources </a></li> <!-- Table with datasets + conversion code + preprocessing code. -->
            <!--<li class="last"><a href="other.html">Other activities </a></li>-->
          </ul>
        </nav>
  </div><div class="clear"></div>
  </div>
  <div class="border-top"></div>

  <!--==============================content====================================-->
  <section id="content">
    <div class="container_12">
      <div class="wrapper border-vert-resouorces">
        <article class="grid_12">
          <h2>Resources</h2>
		<p>A list of resources that we will use during the workshop. These resources include original and publicly available datasets that were standardised according to the CND data structure, as well as original analysis scripts and links to publicly available toolboxes for the analysis of continuous-event neural data.		  
		Note that each dataset should be used according to its own license and should be referenced as indicated by the authors in their original submission.</p>
		<p><b>The main files to download</b>, which can be taken as blueprint for new analyses, are: <a href="https://www.data.cnspworkshop.net/data/CNSP_resources_skeleton.zip">CNSP Resources Skeleton</a>, which contains the folder structure and a basic tutorial code; <a href="https://www.data.cnspworkshop.net/data/CNSP_libs.zip">CNSP Libraries</a>, which should be unzipped in the 'libs' folder; the CND version of <a href="https://data.cnspworkshop.net/data/datasetCND_LalorNatSpeech.zip">Natural speech listening</a> and <a href="https://data.cnspworkshop.net/data/datasetCND_diliBach.zip">Bach piano melodies</a>, which can also be found in the table below; and the CNSP2022 tutorials of interest listed below, which can be run on any CND dataset with minimal code modification.</p>
		<p><a href="resourcesCNSP2021.html">Resources for CSNP 2021 can be found here</a>.</p>
		
		<p style="text-align:justify">
		<h5>Continuous-event Neural Data data format</h5>
		<a href="cndFormat.html">Please check out the CND specifications (Continuous-event Neural Data)</a>. 
		Detailed insights on the format can be found in our <a href="https://docs.google.com/document/d/1z1iYe9dhOnKBZFLVGH5YnabEtODLzzA2PHIObCu8Mlg">Resource preparation guidelines 2022</a> and on the <a href="https://docs.google.com/document/d/1llE0v-e2pdCv5WXoIZzU2V00T2TCElviH7Vu2C8RS50">BYOData Preparation Guidelines</a>. All documents and resources will be refined and publicly shared during and after CNSP2022.		 
		</p><br/>

		
		  <table>
		  	  
		  <tr><th><b><u>CNSP2022 Tutorials</u></b></th> <th></th> <th></th> <th></th> </tr>
		  <tr>
			<td><b>Link</b></td>
			<td><b>Authors</b></td>
			<td><b>Description</b></td>
			<td><b>Relevant tutorial</b></td>
		  </tr>
		  <tr>
			<td><a href="https://www.data.cnspworkshop.net/data/CNSP_resources_skeleton.zip">CNSP Resources Skeleton</a></td>
			<td>CNSP organisers</td>
			<td>Folder structure and a basic example TRF code</td>
			<td>All CNSP tutorials (Day 1)</td>
		  </tr>	
		  <tr>
			<td><a href="https://www.data.cnspworkshop.net/data/CNSP_libs.zip">CNSP Libraries</a></td>
			<td>CNSP organisers</td>
			<td>Unzip them in the 'libs' folder of the skeleton</td>
			<td>All CNSP tutorials (Day 1)</td>
		  </tr>
		  <tr>
			<td><a href="https://www.data.cnspworkshop.net/CNSP2022_videos/resourcePrep.mp4">Video - Resource Preparation</a></td>
			<td>Nidiffer, Di Liberto, and the CNSP2022 participants</td>
			<td>This video will guide you through the CNSP resource preparation (essential practical guidelines from 5min:08s to 16min:08s)</td>
			<td>All CNSP tutorials (Day 1)</td>
		  </tr>
		  <tr>
			  <td><b>Intermediate tutorials:</b>
			    <br/><a href="https://www.data.cnspworkshop.net/data/CNSP2022_intermediate/IntermediateTutorial_StarterPack.zip">Starter pack</a>
			    <br/><a href="https://www.data.cnspworkshop.net/data/CNSP2022_intermediate/MultivarShuffletutorial.zip">Evaluating multivariate models</a>
			    <br/><a href="https://www.data.cnspworkshop.net/data/CNSP2022_intermediate/BandedRidgeTutorial.zip">Banded ridge regression</a>
			    <br/><a href="https://www.data.cnspworkshop.net/data/CNSP2022_intermediate/CCAtutorial_withoutSolutions.zip">CCA</a>
			  </td>
			
			<td>CNSP organisers (Giovanni Di Liberto, Aaron Nidiffer, Nate Zuk)</td>
			<td>Intermediate tutorials. Please unzip in the 'CNSP/tutorials' folder</td>
			<td>Intermediate CNSP tutorials (Day 1)</td>
		  </tr>	  
			  
		  <tr>
			<td><a href="https://www.data.cnspworkshop.net/data/CNSP_2022_tutorial_eelbrain.zip">Eelbrain tutorial (Python)</a></td>
			<td>Joshua Kulasingham</td>
			<td>Demonstrates data preprocessing, forward, and backward modeling with Eelbrain.<br>
				Download the zip file in the link on the left, unzip the file contents, then follow the guide to setup the tutorial: <a href="https://www.data.cnspworkshop.net/data/tutorial_docs/EelbrainTutorial_README.pdf">README</a>.<br>
				For more information, see: <a href="https://ieeexplore.ieee.org/document/9802779">Paper</a> and <a href="https://eelbrain.readthedocs.io/">Eelbrain documentation</a>
			</td>
			<td>TRF Eelbrain tutorial (Day 2)</td>
		  </tr>
		  <tr>
			<td>Envelope decoding using DNNs (Python)</td>
			<td>Mike Thornton</td>
			<td>Compares linear models and deep neural networks (DNN) for envelope decoding from EEG data.<br>
				The tutorial is based in Google Colab. A guide to setup the tutorial can be found in the <a href="https://www.data.cnspworkshop.net/data/tutorial_docs/DNNDecoders_README.pdf">README</a>. <br>
				For more information, see: <a href="https://iopscience.iop.org/article/10.1088/1741-2552/ac7976/meta">Paper</a>
			</td>
			<td>DNN tutorial (Day 2)</td>
		  </tr>
		  <tr>
			<td><b>other tutorials coming soon</b></td>
			<td><b>-</b></td>
			<td><b>-</b></td>
			<td><b>-</b></td>
		  </tr>
		  <!--
          <tr><td><b></b></td><td><b></b></td><td><b></b></td><td><b></b></td></tr>
		  <tr><td><b></b></td><td><b></b></td><td><b></b></td><td><b></b></td></tr>
		  <tr><td><b></b></td><td><b></b></td><td><b></b></td><td><b></b></td></tr>
		  <tr><td><b></b></td><td><b></b></td><td><b></b></td><td><b></b></td></tr>
		  -->
		  
		  <tr><th><b><u>Datasets</u></b></th> <th></th> <th></th> <th></th> </tr>

		  <tr>
			<td><b>Original Link</b></td>
			<td><b>Authors</b></td>
			<td><b>Paper</b></td>
			<td><b>CND data structure</b></td>
		  </tr>
		  <tr>
			<td><a href="https://doi.org/10.5061/dryad.070jc">Speech - multiple EEG datasets</a></td>
			<td>Broderick, Andreson, Di Liberto, Crosse and Lalor</td>
			<td><a href="https://doi.org/10.1016/j.cub.2018.01.080">Current Biology, 2018</a></td>
			<td><a href="https://www.data.cnspworkshop.net/data/datasetCND_LalorNatSpeech.zip">Download Natural Speech</a><br/>
			    <a href="https://www.data.cnspworkshop.net/data/datasetCND_LalorNatSpeechReverse.zip">Download Reverse Natural Speech</a><br/>
			    <a href="https://www.data.cnspworkshop.net/data/LalorCocktailParty.zip">Download Cocktail Party</a></td>
		  </tr>
		  <tr>
			<td><a href="https://datadryad.org/stash/dataset/doi:10.5061/dryad.g1jwstqmh">Bach piano melodies - EEG dataset</a></td>
			<td>Di Liberto, Pelofi, Bianco, Patel, Mehta, Herrero, de Cheveigné, Shamma and Mesgarani</td>
			<td><a href="https://elifesciences.org/articles/51784">eLife, 2020</a></td>
			<td><a href="https://www.data.cnspworkshop.net/data/datasetCND_diliBach.zip">Download Bach Piano Melodies</a></td>
		  </tr>
		  <tr>
			<td><a href="https://doi.org/10.5061/dryad.dbrv15f0j">Music listening/imagery - EEG dataset</a></td>
			<td>Marion, Di Liberto, and Shamma</td>
			<td><a href="https://t.co/h0hyH4JRAt?amp=1">Paper 1</a>; <a href="https://t.co/njKaG7sBlW?amp=1">Paper 2</a></td>
			<td><a href="https://www.data.cnspworkshop.net/data/datasetCND_musicImagery.zip">Download Music Imagery</a></td>
		  </tr>
	 	  <tr>
			<td><a href="https://deepblue.lib.umich.edu/data/concern/data_sets/bg257f92t">Speech listening - EEG dataset</a></td>
			<td>Brennan and Hale </td>
			<td><a href="https://doi.org/10.1371/journal.pone.0207741">PLoS ONE, 2019</a></td>
			<td><a href="https://www.data.cnspworkshop.net/data/AliceSpeech.zip">Download Speech Listening</td>
		  </tr>
		  <tr>
			<td><a href="https://zenodo.org/record/3997352#.X0fP1sgza5g">Auditory Attention - EEG dataset</a></td>
			<td>Das, Francart, Bertrand</td>
			<td><a href="https://pubmed.ncbi.nlm.nih.gov/27618842/">J. Neural Eng, 2016</a></td>
			<td><a href="https://www.data.cnspworkshop.net/data/AAD_KULeuven.zip">Download Auditory Attention</a></td>
		  </tr>
		  <!--<tr>
			<td><a href="https://doi.org/10.5061/dryad.070jc">Preprocessed Speech EEG dataset</a></td>
			<td>Broderick, Andreson, Di Liberto, Crosse and Lalor</td>
			<td><a href="https://doi.org/10.1016/j.cub.2018.01.080">Current Biology, 2018</a></td>
			<td><a href="https://www.data.cnspworkshop.net/data/dataSub10_proc.mat">Download CND</a></td>
		  </tr>	  
		  <tr>
			<td>Dataset</td>
			<td>Authors</td>
			<td>Paper</td>
			<td>Conversion script</td>
		  </tr>
		  <tr>
			<td>Dataset</td>
			<td>Authors</td>
			<td>Paper</td>
			<td>Conversion script</td>
		  </tr>
		  <tr>
			<td>Dataset</td>
			<td>Authors</td>
			<td>Paper</td>
			<td>Conversion script</td>
		  </tr>-->
			  
		  <tr><th><b><u>Toolboxes</u></b></th> <th></th> <th></th> <th></th> </tr>

		  <tr>
			<td><b>Link</b></td>
			<td><b>Authors</b></td>
			<td><b>Paper</b></td>
			<td><b>Description</b></td>
		  </tr>
		  <tr>
			<td><a href="https://github.com/mickcrosse/mTRF-Toolbox">mTRF-Toolbox<br/>(Domain-specific)</a></td>
			<td>Crosse, Di Liberto, Bednar and Lalor</td>
			<td><a href="https://www.frontiersin.org/articles/10.3389/fnhum.2016.00604/full">Front Hum Neurosci 2016</a></td>
			<td>A MATLAB toolbox for relating neural signals to continuous stimuli.</td>
		  </tr>
		  <tr>
			<td><a href="https://eelbrain.readthedocs.io/">Eelbrain<br/>(Domain-specific)</a></td>
			<td>Brodbeck, Das, Kulasingham</td>
			<td><a href="https://www.biorxiv.org/content/10.1101/2021.08.01.454687v1">Paper</a><br/><a href="https://www.data.cnspworkshop.net/CNSP2021_slides/CNSP2021_ChristianBrodbeckSlides.pdf">CNSP2021 slides</a></td>
			<td>A Python toolbox for relating neural signals to continuous stimuli.</td>
		  </tr>
	          <tr>
			<td><a href="http://audition.ens.fr/adc/NoiseTools/">NoiseTools</a></td>
			<td>Alain de Cheveigné</td>
			<td>Multiple references. See <a href="http://audition.ens.fr/adc/NoiseTools/">here</a></td>
			<td>a Matlab toolbox to denoise and analyze multichannel electrophysiological data, such as from EEG, MEG, electrode arrays, optical imaging, or fMRI.</td>
		  </tr>
	          <tr>
			<td><a href="https://sccn.ucsd.edu/eeglab/index.php">EEGLAB<br/>(General purpose)</a></td>
			<td>Delorme and Makeig, J. Neurosci Methods, 2004</td>
			<td><a href="https://www.sciencedirect.com/science/article/pii/S0165027003003479">Paper</a></td>
			<td>EEGLAB is an interactive Matlab toolbox for processing continuous and event-related EEG, MEG and other electrophysiological data.</td>
		  </tr>
		  <tr>
			<td><a href="https://mne.tools/stable/index.html">MNE<br/>(General purpose)</a></td>
			<td>Gramfort et al. 2013</td>
			<td><a href="https://doi.org/10.3389/fnins.2013.00267">Paper</a></td>
			<td>Open-source Python package for exploring, visualizing, and analyzing human neurophysiological data: MEG, EEG, sEEG, ECoG, NIRS, and more.</td>
		  </tr>	  
			  
			  
			  
		  <tr><th><b><u>Other Useful References</u></b></th> <th></th> <th></th> <th></th> </tr>

		  <tr>
			<td><b>Link</b></td>
			<td><b>Authors</b></td>
			<td><b>Description</b></td>
			<td><b>Year</b></td>
		  </tr>
		  <!--<tr>
			<td><a href="https://www.frontiersin.org/articles/10.3389/fnhum.2016.00604/full">Paper</a></td>
			<td>Crosse, Di Liberto, Bednar, Lalor</td>
			<td>"The multivariate temporal response function (mTRF) toolbox: a MATLAB toolbox for relating neural signals to continuous stimuli", <i>Frontiers in Human Neuroscience</i></td>
			<td>2016</td>
		  </tr>-->
		  <tr>
			<td><a href="https://doi.org/10.3389/fnins.2021.705621">Paper</a></td>
			<td>Crosse, Zuk, Di Liberto, Nidiffer, Molholm, Lalor</td>
			<td>Using TRFs in applied research. "Linear Modeling of Neurophysiological Responses to Naturalistic Stimuli: Methodological Considerations for Applied Research", <i>Frontiers in Neuroscience</i></td>
			<td>2021</td>
		  </tr>
		  <!--<tr>
		      <td><a href="">Paper</a></td>
		      <td>Nunez-Elizalde, Huth, Gallant</td>
		      <td>"Voxelwise encoding models with non-spherical multivariate normal priors", <i>Neuroimage</i></td>
		      <td>2019</td>
		  </tr>-->
		  <tr>
			<td><a href="https://www.sciencedirect.com/science/article/pii/S1053811920310715">Paper</a></td>
			<td>Di Liberto, Nie, Yeaton, Khalighinejad, Shamma, Mesgarani</td>
			<td>This study used multivariate encoding TRFs. "Neural representation of linguistic feature hierarchy reflects second-language proficiency", <i>Neuroimage</i></td>
			<td>2021</td>
		  </tr>
		  <!--<tr>
			<td><a href="https://elifesciences.org/articles/51784">Paper</a></td>
			<td>Di Liberto, Pelofi, Bianco, Patel, Mehta, Herrero, de Cheveigne, Shamma, Mesgarani</td>
			<td>TRFs with music stimuli (EEG and ECoG). "Cortical encoding of melodic expectations in human temporal cortex", <i>eLife</i></td>
			<td>2020</td>
		  </tr>	-->
		  <tr>
			<td><a href="https://ieeexplore.ieee.org/document/9802779">Paper</a></td>
			<td>Kulasingham, Simon</td>
			<td>Compares methods for computing TRFs and their estimates of evoked responses. "Algorithms for estimating time-locked neural response components in cortical processing of continuous speech"</td>
			<td>2022</td>
		  </tr>
		  <tr>
			<td><a href="https://iopscience.iop.org/article/10.1088/1741-2552/ac7976/meta">Paper</a></td>
			<td>Thornton, Mandic, Reichenbach</td>
			<td>Examines DNNs for envelope decoding from EEG. "Robust decoding of the speech envelope from EEG recordings through deep neural networks"</td>
			<td>2022</td>
		  </tr>
		  <tr>
			<td><a href="https://computationalaudiology.com/resources/">Computational Audiology Resources</a></td>
			<td>
Barbour, Hohmann, Ntlhakana, Zeng, Buhl, Goehring, Warzybok, Wasmann</td>
			<td>This initiative strives to highlight recent examples that illustrate the potential of a computational approach to audiology. computationalaudiology.com aims to become a central hub for sharing resources that are useful for researchers and clinicians.</td>
			<td>2020-current</td>
		  </tr>		  

		</table>


        </article>


      </div>
    </div>
  </section>
</div>
<!--==============================footer================================-->
<!-- particles.js container -->
  <footer>

  <hr WIDTH="58.5%" ALIGN="LEFT">
  <hr WIDTH="58.5%" ALIGN="RIGHT">
  <div class="container_12">


    <div class="wrapper">
      <article class="grid_8">
          <u>Organisers</u><br/>
		  Giovanni Di Liberto, PhD<br/>
		  Nathaniel Zuk, PhD<br/>
		  Mick Crosse, PhD<br/>
		  Aaron Nidiffer, PhD<br/>
		  Giorgia Cantisani, PhD<br/>
		  Stephanie Haro<br/>
          CNSPWorkshop /at/ gmail /dot/ com
      </article>
      <article class="grid_4">
        <div>
			<a href="https://github.com/CNSP-Workshop"><font color="BBBBBB">GitHub</font></a><br/>
			<a href="https://twitter.com/CnspWorkshop"><font color="BBBBBB">Twitter</font></a><br/>
			<br/>
		</div>
		<div class="privacy">2021 &nbsp;All rights reserved <br />
          <!-- Do not remove -->Website by <a href="https://diliberg.net" title="Giovanni Di Liberto">Giovanni Di Liberto</a><!-- end --></div>
		  <!-- Do not remove -->Original design by <a href="http://www.libdesigner.com/2012/08/24/free-animal-skin-and-fur-textures-for-your-projects/" title="Animal Fur Textures">Animal Fur Textures</a><!-- end --><br/>
      </article>
    </div>
  </div>
</footer>
</div>
</body>
</html>
