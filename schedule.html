<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>CNSP-Workshop 2021</title>
<meta name="keywords" content="Cognitive, Natural, Sensory Processing, EEG, Neural, Speech, Music, Perception, Machine Learning" />
<meta name="description" content="" />
<meta name="author" content="Giovanni M. Di Liberto">
<link rel="stylesheet" href="styles.css" />
</head>
<body id="page2">
<div class="bg-main">
  <!--==============================header=================================-->
 <div class="header_bg">
  <div class="header">

      <div class="main">
        <h1><a href="#"><img src="images/CNSP_Logo.gif" alt="" style="height:150px;"></a></h1>

	</div>
        <nav>
          <ul class="sf-menu">
            <li><a href="index.html">Home </a></li>
            <li><a href="registration.html">Registration </a></li>
            <li class="current"><a href="schedule.html">Schedule </a></li>
			<!-- <li><a href="aboutWorkshop.html">About workshop </a></li> <! Goal. Why now. Code of conduct. > -->
            <li class="last"><a href="resources.html">Resources </a></li> <!-- Table with datasets + conversion code + preprocessing code. -->
            <!--<li class="last"><a href="other.html">Other activities </a></li>-->
          </ul>
        </nav>
  </div>
  <div class="clear"></div>
  </div>

  <div class="border-top"></div>
  <!--<hr WIDTH="58.5%" ALIGN="LEFT">-->
  <!--<hr WIDTH="58.5%" ALIGN="RIGHT">-->
  <!--<hr WIDTH="65%" ALIGN="CENTER">-->

  <!--==============================content====================================-->
  <section id="content">
    <div class="container_12">
      <div class="wrapper border-vert-resouorces">
        <article class="grid_12">
          <h2>Cognition and Natural Sensory Processing Workshop (CNSP)</h2>
	      <h5>2-4 August 2021</h5>
          <div class="wrapper">
            <!--<figure class="img-indent3"><img src="images/cv1.jpg" alt="" /></figure>-->
            <br>
            <h3>Schedule</h3><br/>
            <div class="inner-2">
            <h6><i>Day 1 (2nd August 2021)</i></h6>
            <p>
                <u>Session 1 (2pm GMT)</u>: Investigating auditory processing with natural sound listening paradigms
                <ul>
					<li>The CNSP-Workshop: What, why, and how</li>
					<li><b>Jonas Obleser</b> &mdash; Neural tracking of continuous sensory stimuli</li>
					<li><b>Edmund Lalor</b> &mdash; The Temporal Response Function: Concept and basic research</li>
					<li><i>Hands-on tutorial: </i> Demonstration and practice. Environment setup.</li>
                </ul>
            </p>

            <p>
                <u>Session 2 (4.30pm GMT)</u>: Encoding and Decoding models for neural signal analysis: Use and interpretation
                <ul>
					<li><b>Lien Decruy & Joshua Kulasingham</b> &mdash; Encoding models</li>
					<li><b>Laura Gwilliams</b> &mdash; Decoding models</li>
					<li><b>Aaron Nidiffer</b> &mdash; Stimulus feature extraction</li>
					<li><i>Hands-on tutorial:</i> Demonstration and practice + Details on the mini-project</li>
				</ul>
            </p>

            <h6><i>Day 2 (3rd August 2021)</i></h6>
            <p>
                <u>Mini-session (3:30pm GMT)</u>: <b>Christian Brodbeck</b> &mdash; Investigating speech processing with Python and Eelbrain
            </p>

            <p>
                <u>Session 3 (4pm GMT)</u>: TRFs in applied research: Case studies
				<ul>
					<li><b>Sarah Jessen</b> &mdash; Speech sound perception in infants</li>
					<li><b>Elana Zion-Golumbic</b> &mdash; Audio-visual perception</li>
					<li><b>Jens Hjortkjær</b> &mdash; Auditory TRFs in ageing and hearing impaired cohorts</li>
					<li>General discussion</li>
				</ul>
            </p>

            <h6><i>Day 3 (4th August 2021)</i></h6>
            <p>
                <u>Session 4 (4pm GMT)</u>:
				<ul>
					<li>Q/A Session(s)</li>
					<li>Closing remarks</li>
				</ul>
            </p>

            </div>
          </div>
          <br>
          <h3>Speaker Line-Up</h3>
          <div class="wrapper">
              <div class="inner-2" style="overflow:auto;clear:both;margin-top:20px">
                  <h6>Jonas Obleser, PhD</h6>
                  <p><img src="images/speaker_pics/jonas_obleser.jpg" border="2px" height="150" width="150" style="float:left;margin-right:10px;">Jonas Obleser researches processes of perception and listening using methods from the cognitive neurosciences. After studying and obtaining his doctorate in psychology at the University of Konstanz, he worked at the Institute of Cognitive Neuroscience, University College London, and at the Max Planck Institute in Leipzig, where he established the research group <a href="http://auditorycognition.com">"Auditory Cognition"</a>. Since 2016, he has held the Chair of Physiological Psychology at the University of Lübeck, Germany. His current research interests include the dynamic changes of brain activity in perception and cognition, and how these processes interact during listening. His research is currently funded by the European Research Council (ERC), among others.</p>
              </div>

              <div class="inner-2" style="overflow:auto;clear:both;margin-top:20px">
                  <h6>Edmund Lalor, PhD</h6>
                  <p><img src="images/speaker_pics/edmund_lalor.jpg" height="150" width="150" style="float:left;margin-right:10px;">Ed Lalor is an Associate Professor in the Departments of Biomedical Engineering and Neuroscience at the University of Rochester. The major goal of his lab's work is to develop novel methodologies for neuroscience research and to apply those methodologies to study human sensory, perceptual and cognitive processing in health and disease. This work has led to improved flexibility in the design of experiments aimed at the neurophysiological correlates of attention and multisensory integration and some novel insights into the neurophysiological specificity of early sensory deficits in disorders such as autism and schizophrenia. In addition, it has provided researchers with the ability to obtain temporally detailed responses to stimuli that are much more naturalistic than those that are often used in EEG experiments. This includes significant recent efforts aimed at understanding the neurophysiology of receptive speech processing and how it is affected by attention and multisensory input. Ongoing work in the lab seeks to continue to develop sophisticated computational modelling frameworks to allow for greater interpretation of noninvasively recorded brain data and to translate these novel modelling methods into impactful research in clinical populations.</p>
              </div>

              <div class="inner-2" style="overflow:auto;clear:both;margin-top:20px">
                  <h6>Lien Decruy, PhD</h6>
                  <p><img src="images/speaker_pics/lien_decruy.jpg" height="137" width="150" style="float:left;margin-right:10px;">Her background is in Audiology and Neuroscience. In 2014 and 2015, she obtained her Master’s degree in Speech Therapy and Audiology Sciences as well as a Postgraduate training in Audiology and Hearing Aid Fitting, both at the KU Leuven in Belgium. Between 2015 and 2019, she was a PhD-student at the Research Group Experimental ORL (ExpORL, Department of Neurosciences, KU Leuven) under supervision of Prof. Dr. Ir. Tom Francart. In her PhD thesis, she focused on measuring neural tracking of natural speech in normal-hearing and hearing impaired listeners of different ages, providing several first steps towards an objective (EEG) measure of speech understanding. After obtaining her PhD, Lien continued in the field of neuroscience and started in January 2020 as a postdoctoral researcher at the university of Maryland (USA), in the lab of Dr. Jonathan Simon, Dr. Samira Anderson and Dr. Stefanie Kuchinsky. Her research mainly involved the evaluation of a new auditory‐cognitive training program for older adults, based on EEG/MEG responses to natural speech (using decoding and encoding models) and pupillometry. In June 2021, Lien started working as a study coordinator and research assistant of the medical ethical committee in the hospital AZ Groeninge Kortrijk, Belgium.</p>
              </div>

              <div class="inner-2" style="overflow:auto;clear:both;margin-top:20px">
                  <h6>Joshua Kulasingham</h6>
                  <p><img src="images/speaker_pics/joshua_kulasingham.jpg" height="150" width="150" style="float:left;margin-right:10px;">Joshua P. Kulasingham is a PhD student in Dr. Jonathan Z. Simon’s lab at the Department of Electrical and Computer Engineering, University of Maryland, College Park, USA. His research explores time-locked neural responses to speech, typically in a cocktail party paradigm, using MEG. He works with encoding models (Temporal Response Functions) that estimate cortical responses to relevant features of continuous speech. He has investigated high frequency time-locked responses to speech, and cortical processing of spoken sentences and equations.</p>
              </div>

              <div class="inner-2" style="overflow:auto;clear:both;margin-top:20px">
                  <h6>Laura Gwilliams, PhD</h6>
                  <p><img src="images/speaker_pics/laura_gwilliams.jpg" height="135" width="150" style="float:left;margin-right:10px;">Laura Gwilliams received her PhD in Psychology with a focus in Cognitive Neuroscience from New York University in May 2020. Currently she is a post-doctoral researcher at UCSF, using MEG and ECoG data to understand how linguistic structures are parsed and composed while listening to continuous speech. The ultimate goal of Laura’s research is to describe speech comprehension in terms of what operations are applied to the acoustic signal, which representational formats are generated and manipulated (e.g. phonetic, syllabic, morphological), and under what processing architecture.</p>
              </div>

              <div class="inner-2" style="overflow:auto;clear:both;margin-top:20px">
                  <h6>Aaron Nidiffer, PhD</h6>
                  <p><!--<img src="images/speaker_pics/aaron_nidiffer.jpg" height="150" width="150" style="float:left;margin-right:10px;">-->Aaron Nidiffer, Ph.D. is a Postdoctoral Research Associate in the Computational Cognitive Neurophysiology Laboratory of Dr. Edmund Lalor at the University of Rochester. Before moving to Rochester, he completed his doctorate at Vanderbilt University in Nashville, TN under the supervision of Dr. Mark Wallace. His interests lie broadly in speech processing and multisensory perception. His dissertation research focused on how correlations in the audiovisual sensory environment affect perception and object formation. As a postdoc, he is using both complex artificial (e.g., stochastic figure-ground) and natural (e.g., speech) signals to explore the neural underpinnings of audiovisual binding and the unique contributions of visual signals to language perception.</p>
              </div>

              <div class="inner-2" style="overflow:auto;clear:both;margin-top:20px">
                  <h6>Christian Brodbeck, PhD</h6>
                  <p><img src="images/speaker_pics/christian_brodbeck.jpg" height="150" width="150" style="float:left;margin-right:10px;">Christian Brodbeck studies the neural basis of language, and speech processing in particular. When humans listen to speech, the acoustic signal that enters the ears is a complex pattern of air pressure fluctuations. Yet, listeners intuitively and almost instantaneously experience meaning in these sounds. His research focuses on the transformations that happen in the brain to enable this. To study this, he mainly uses MEG and EEG with reverse correlation. Reverse correlation allows us to think of brain responses as a continuous transformation of the speech signal, rather than relying on pre-defined events in the stimuli. It also allows us to disentangle responses related to different levels of processing, such as the formation of auditory and lexical representations. He uses Python to develop tools to make this research possible, and many of those tools are available in the open source libraries <a href="https://mne.tools/stable/index.html">MNE-Python</a> and <a href="https://eelbrain.readthedocs.io/en/stable/">Eelbrain.</a></p>
              </div>

              <div class="inner-2" style="overflow:auto;clear:both;margin-top:20px">
                  <h6>Sarah Jessen, PhD</h6>
                  <p><img src="images/speaker_pics/sarah_jessen.jpg" height="150" width="150" style="float:left;margin-right:10px;">Sarah has a background in neuroscience and psychology and currently runs the babylab at the University of Lübeck in Germany. Her research focus is on the neural mechanisms of social and emotional processing in the first year of life, and she uses predominantly EEG but also eyetracking and fNIRS.</p>
              </div>

              <div class="inner-2" style="overflow:auto;clear:both;margin-top:20px">
                  <h6>Elana Zion Golumbic, PhD</h6>
                  <p><img src="images/speaker_pics/elana_zion_golumbic.jpg" height="137" width="150" style="float:left;margin-right:10px;">Dr. Elana Zion Golumbic is the head of the Human Brain Dynamics Laboratory at the Multidisciplinary Center for Brain Research at Bar Ilan University. Her research focuses on studying how the brain processes dynamic information under real-life conditions and environments. Specifically, she seeks to understand the neural mechanisms underlying the processing of natural continuous stimuli, with a specific interest in real-life speech.  Among the questions investigated in her lab are: <i>How is information from different senses integrated on-line? How does the brain encode competing input from our rich multisensory environments? And what are the mechanisms for managing attention in noisy and cluttered environments?</i> Research in her lab utilizes a range of techniques for recording electric and magnetic signals from the human brain (EEG, MEG and ECoG), alongside a variety of psychophysical tools (eye-tracking, virtual reality, psychoacoustics). Dr. Zion Golumbic is an expert in applying advanced signal processing tools and machine-learning algorithms to human-recorded brain signals, an approach that is critical for furthering our understanding of the underlying neural code and the link between brain operations and human behavior.</p>
              </div>

              <div class="inner-2" style="overflow:auto;clear:both;margin-top:20px">
                  <h6>Jens Hjortkjær, PhD</h6>
                  <p><img src="images/speaker_pics/jens_hjortkjaer.jpg" height="150" width="150" style="float:left;margin-right:10px;">Jens Hjortkjær is currently a Senior Researcher in auditory cognitive neuroscience at the Hearing Systems group at the Technical University of Denmark (DTU), and also at the Danish Research Centre for Magnetic Resonance (DRCMR) at Copenhagen University Hospital Hvidovre. He leads the Auditory Cognitive Neuroscience group at DTU Health Technology. His main focus is on using neuroimaging, psychophysics, and computational modelling to understand the auditory system in normal and hearing-impaired listeners. JH studied cognitive psychology and music at Copenhagen University from where he received his PhD in 2011. After joining the DRCMR, he worked on using functional MRI to study the auditory cortex and effects of attention on cortical sound representations. He joined DTU as a postdoc in 2013 working on decoding auditory attention from speech-EEG, and worked on cognitive control of hearing aids in the H2020 COCOHA project. He is currently coordinator of the UHEAL project that combines MRI and neurophysiology to investigate auditory nerve degeneration and its consequences in central brain processing and behaviour.</p>
              </div>
          </div>
        </article>

<!--

        <article class="grid_4">
          <div class="inner-1">
            <h2>Key dates</h2>
			<div class="box-team">
              <div class="wrapper p18">
                <div class="overflow">
                  <h5>2 August 2021: Day 1</h5>
				  <h5>3 August 2021: Day 2</h5>
				  <h5>4 August 2021: Q/A session!</h5>
                </div>
              </div>
			  <div class="wrapper p18">
                <div class="overflow">
                  <h5>1 July 2021: Registration deadline</h5>
                </div>
              </div>
			  <div class="wrapper p18">
                <div class="overflow">
                  <h5>15 May 2021: Provisional speaker lineup</h5>
                </div>
              </div>
            </div>
			<br/><br/>

            <a class="twitter-timeline" data-chrome="transparent" data-theme="light" href="https://twitter.com/CnspWorkshop?ref_src=twsrc%5Etfw">Tweets by CnspWorkshop</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



          </div>
        </article>
-->
      </div>
    </div>
  </section>
</div>

<!--==============================footer================================-->
<!-- particles.js container -->
  <footer>

  <hr WIDTH="58.5%" ALIGN="LEFT">
  <hr WIDTH="58.5%" ALIGN="RIGHT">
  <div class="container_12">


    <div class="wrapper">
      <article class="grid_8">
          Organisers<br/>
		  Giovanni Di Liberto, PhD<br/>
		  Nathaniel Zuk, PhD<br/>
		  Michael Crosse, PhD<br/>
          CNSPWorkshop /at/ gmail /dot/ com
      </article>
      <article class="grid_4">
        <div>
			<a href="https://github.com/CNSP-Workshop/"><font color="BBBBBB">GitHub</font></a><br/>
			<a href="https://twitter.com/cnsp-workshop"><font color="BBBBBB">Twitter</font></a><br/>
			<br/>
		</div>
		<div class="privacy">2021 &nbsp;All rights reserved <br />
          <!-- Do not remove -->Website by <a href="https://diliberg.net" title="Giovanni Di Liberto">Giovanni Di Liberto</a><!-- end --></div>
		  <!-- Do not remove -->Original design by <a href="http://www.libdesigner.com/2012/08/24/free-animal-skin-and-fur-textures-for-your-projects/" title="Animal Fur Textures">Animal Fur Textures</a><!-- end --><br/>
      </article>
    </div>
  </div>
</footer>
</div>
</body>
</html>
